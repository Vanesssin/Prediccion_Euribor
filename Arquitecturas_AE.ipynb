{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arquitecturas_AE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gRd189Yo-I4b",
        "GWioufveenqP",
        "5hNrUDVkUWIf",
        "hKnqZNtewnz-",
        "1GY7QMZOOpoR"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWna63lTD0sFiuYNOVkSe3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vanesssin/Prediccion_Euribor/blob/main/Arquitecturas_AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBPbiTIb6PA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spzAaqIw7_lU"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neR8cHdOtzvW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTsPngstzvX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmxlHk9btzva"
      },
      "source": [
        "PATH=####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lUCdtztzva"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "SMALL_SIZE = 8 +10\n",
        "MEDIUM_SIZE = 10 +10\n",
        "BIGGER_SIZE = 12 +10\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)   # controls default text sizes \n",
        "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the axes title \n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE) # fontsize of the x and y labels \n",
        "plt.rc('xtick', labelsize=SMALL_SIZE) # fontsize of the tick labels \n",
        "plt.rc('ytick', labelsize=SMALL_SIZE) # fontsize of the tick labels \n",
        "plt.rc('legend', fontsize=SMALL_SIZE) # legend fontsize \n",
        "plt.rc('figure', titlesize=BIGGER_SIZE) # fontsize of the figure title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBB_3r13kK4Z"
      },
      "source": [
        "### SOLO EJECUTAR EL PRIMER DIA DE LOS BARRIDOS - CUADNO EL PICKEL NO ESTÁ GENERADO\n",
        "import pickle\n",
        "\n",
        "#res_barrido={}\n",
        "#pickle_out = open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\",\"wb\")\n",
        "#pickle.dump(res_barrido, pickle_out)\n",
        "#pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgNM6UTdke9R"
      },
      "source": [
        "# Codigo para recuperar el pickle de los resultados\n",
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "    res_barrido = pickle.load(input_file)\n",
        "print(len(res_barrido.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRd189Yo-I4b"
      },
      "source": [
        "# Load and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR04kfYD7xir"
      },
      "source": [
        "Data from: https://www.quandl.com/data/BOF/QS_D_IEUTIO3M-EURIBOR-3-Months-Daily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_hONBA-7xir"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/My Drive/TFM_VIU/data/QS_D_IEUTIO3M_new.json\") as file:\n",
        "  data=json.load(file)\n",
        "data_source2= pd.DataFrame(data['dataset']['data'])\n",
        "data_source2.columns=['Date','value']\n",
        "data_source2.index=pd.to_datetime(data_source2['Date'].astype(str))\n",
        "data_source2.sort_index(inplace=True,ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho5AOANp7xit"
      },
      "source": [
        "### Check NA data and be sure not weekend timestamp values\n",
        "def check_delta_time(data_serie, n=5):\n",
        "  delta_time=[]\n",
        "  for i in range(5,len(data_serie)):\n",
        "    delta_time.append((data_serie[i]-data_serie[i-n]).days)\n",
        "  return np.asarray(delta_time)\n",
        "\n",
        "\n",
        "data_source2['day_of_week']=data_source2.index.dayofweek.values\n",
        "check_1= check_delta_time(data_source2.index, n=5)\n",
        "data_source2['check_diffs']=np.append(np.nan*np.ones(5), check_1)\n",
        "\n",
        "# Fix the same time period (interpolation and imputation of NA values)\n",
        "first_day=data_source2.index[0]\n",
        "last_day=data_source2.index[-1]\n",
        "\n",
        "print('first recording: ', first_day)\n",
        "print('last recording: ', last_day)\n",
        "\n",
        "# new range\n",
        "new_range = pd.date_range(first_day, last_day, freq='D')\n",
        "\n",
        "# resample\n",
        "data_source2=data_source2.reindex(new_range)\n",
        "#Remove week-end days\n",
        "data_source2['day_of_week']=data_source2.index.dayofweek.values\n",
        "data_source2=data_source2[(data_source2['day_of_week']!=5)&(data_source2['day_of_week']!=6)]\n",
        "\n",
        "#Interpolate values\n",
        "data_source2['value_interp']=data_source2['value'].interpolate().apply(lambda x: np.round(x,decimals=3))\n",
        "\n",
        "#Temporal serie to model\n",
        "data=data_source2['value_interp'].reset_index(drop=True)\n",
        "data=data.values[:,None]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyNvc7XO8L58"
      },
      "source": [
        "#Representar data original (temporal serie)\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.plot(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lR428H8L59"
      },
      "source": [
        "#Escalar variables\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-DdIuOd8L5-"
      },
      "source": [
        "#Representar data_scaled\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.plot(data_scaled)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u9xXMdf8hOP"
      },
      "source": [
        "np.min(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWioufveenqP"
      },
      "source": [
        "# General functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mghHLzdn9sQ9"
      },
      "source": [
        "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif out_end_ix > len(sequence):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn np.array(X), np.array(y)\n",
        " \n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPs1X9YrApwH"
      },
      "source": [
        "def theil (y_test, y_pred, X_test,PH):\n",
        "  num= (y_test-y_pred)**2\n",
        "  den=(y_test-X_test[:,-PH:])**2\n",
        "\n",
        "  return np.sum(num)/np.sum(den)\n",
        "\n",
        "#If THEIL = 1, the model has a performance equal to a trivial prediction, \n",
        "#if THEIL > 1, the performance is worse than a trivial prediction; \n",
        "#and if THEIL < 1, the performance is better than a trivial prediction."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUERRFEzApwP"
      },
      "source": [
        "def pocid(inv_yhat,inv_y):\n",
        "\n",
        "  return 100*np.sum(np.diff(inv_yhat,1, axis=0).ravel()*np.diff(inv_y,1, axis=0).ravel()<0)/len(np.diff(inv_yhat,1, axis=0).ravel())\n",
        "\n",
        "#The Percentage of Correct (up/down) Directional Prediction\n",
        "#POCID, shown in Eq. (9), measures the percentage of accuracy\n",
        "#relating to the series trend."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZIZ9bOUZFKl"
      },
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\t\"\"\"\n",
        "\tFrame a time series as a supervised learning dataset.\n",
        "\tArguments:\n",
        "\t\tdata: Sequence of observations as a list or NumPy array.\n",
        "\t\tn_in: Number of lag observations as input (X).\n",
        "\t\tn_out: Number of observations as output (y).\n",
        "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
        "\tReturns:\n",
        "\t\tPandas DataFrame of series framed for supervised learning.\n",
        "\t\"\"\"\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNrUDVkUWIf"
      },
      "source": [
        "# Parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlSM5guuGe1l"
      },
      "source": [
        "## Definir variables train/val/test\n",
        "N_FEATURES=1\n",
        "OPTION='A'#'B'\n",
        "if OPTION =='A':\n",
        "  per_train=0.6\n",
        "  per_val=0.2\n",
        "  per_test=0.2\n",
        "else:\n",
        "  per_train=0.7\n",
        "  per_val=0.2\n",
        "  per_test=0.1\n",
        "\n",
        "\n",
        "# split dataset (train/val) \n",
        "per_split=0.75\n",
        "\n",
        "# define seed\n",
        "import random\n",
        "random.seed(44)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeATex8Se3e_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbxSVPShjpF"
      },
      "source": [
        "# Model arquitectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7-3FsNpfPmB"
      },
      "source": [
        "# univariate multi-step encoder-decoder lstm example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAqgubsxeL8G"
      },
      "source": [
        "## model 1\n",
        "# define the model\n",
        "def model_01(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(50, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        "## model 2\n",
        "# define the model\n",
        "def model_02(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        "## model 3\n",
        "# define the model\n",
        "def model_03(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, activation='relu', input_shape=(N_LAGS, N_FEATURES)))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        "## model 4\n",
        "# define the model\n",
        "def model_04(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, activation='relu', input_shape=(N_LAGS, N_FEATURES),return_sequences=True))\n",
        "  model.add(LSTM(64, activation='relu'))\n",
        "  model.add(Dense(32))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(32)))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        "## model 5\n",
        "# define the model\n",
        "def model_05(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, activation='relu', input_shape=(N_LAGS, N_FEATURES), return_sequences=True,dropout=0.2))\n",
        "  model.add(LSTM(64, activation='relu'))\n",
        "  model.add(Dense(32))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(32)))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        "\n",
        "## model 6\n",
        "# define the model\n",
        "def model_06(N_LAGS, N_FEATURES, PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, activation='relu', input_shape=(N_LAGS, N_FEATURES), return_sequences=True,dropout=0.2))\n",
        "  model.add(LSTM(64, activation='relu'))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dra11BJtz4FT"
      },
      "source": [
        " ### Using Stacked LSTM\n",
        "## model 10\n",
        "# define the model\n",
        "def model_10(input_shape,PH):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, return_sequences=True,input_shape=input_shape))\n",
        "  model.add(LSTM(64, return_sequences=False))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Dense(8))\n",
        "  model.add(Dense(PH))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        " ## model 11\n",
        "# define the model\n",
        "def model_11(input_shape,PH):\n",
        "  dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, return_sequences=True,input_shape=input_shape, dropout=0.2))\n",
        "  model.add(LSTM(64, return_sequences=False))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(8))\n",
        "  model.add(Dense(PH))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        " ## model 12\n",
        "# define the model\n",
        "def model_12(input_shape,PH):\n",
        "  dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, return_sequences=False,input_shape=input_shape))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Dense(8))\n",
        "  model.add(Dense(PH))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n",
        "\n",
        " ## model 13\n",
        "# define the model\n",
        "def model_13(input_shape,PH):\n",
        "  dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(64, return_sequences=False,input_shape=input_shape, dropout=0.2))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(8))\n",
        "  model.add(Dense(PH))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkiTOIGxwTuy"
      },
      "source": [
        "# Barrido N_LAGS and PH modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMzzFFtgNaT"
      },
      "source": [
        "## BARRIDO DE PARÁMETROS\n",
        "\n",
        "LIST_PH=[1,3,7,15]\n",
        "LIST_N_LAGS=[180,90,30,15]\n",
        "#LIST_N_LAGS=LIST_N_LAGS[1:]\n",
        "\n",
        "# PREDICTION PARAMS (no- change!)\n",
        "#PH= 3\n",
        "#N_LAGS= 180#30\n",
        "MAX_EPOCHS = 20\n",
        "BATCH_SIZE=32\n",
        "PATIENCE=2\n",
        "OPTIMIZER='adam'\n",
        "lOSS_METRIC='mae'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Q60ahh6Wzo"
      },
      "source": [
        "#Cargar librerías\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Bidirectional\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5135Qemy0K"
      },
      "source": [
        "## mAIN PROCESS\n",
        "\n",
        "def main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS):\n",
        "  #Prepar data k-validation\n",
        "  # PREDICTION PARAMS\n",
        "  OUT_STEPS=PH\n",
        "  num_features=N_FEATURES\n",
        "  #\n",
        "\n",
        "  #porcessed signal\n",
        "  X, y = split_sequence(list(data_scaled), N_LAGS, PH)\n",
        "  data_transf=series_to_supervised(data_scaled, n_in=N_LAGS, n_out=PH, dropnan=True)\n",
        "\n",
        "  ## Prepare data for 4-fold validation (method 1)\n",
        "\n",
        "  # division train/val/test\n",
        "  len_h=len(X)\n",
        "  dict_kfold={'4': {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)} }\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['4']['Test'])\n",
        "  dict_kfold ['3']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['3']['Test'])\n",
        "  dict_kfold['2']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['2']['Test'])\n",
        "  dict_kfold ['1']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "\n",
        "\n",
        "\n",
        "  # Model architecture \n",
        "  #MODEL= model_1\n",
        "  #MODEL_LABEL='M1'\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]={}\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Train']=[]\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Val']=[]\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Test']=[]\n",
        "\n",
        "  #TRAINING MODEL\n",
        "  cvscores_val = []\n",
        "  cvscores_test=[]\n",
        "  iter=0\n",
        "\n",
        "\n",
        "  Train_RMSE=[]\n",
        "  Train_MAE=[]\n",
        "  Train_MAPE=[]\n",
        "  Val_RMSE=[]\n",
        "  Val_MAE=[]\n",
        "  Val_MAPE=[]\n",
        "  Test_RMSE=[]\n",
        "  Test_MAE=[]\n",
        "  Test_MAPE=[]\n",
        "\n",
        "\n",
        "  for k, v in dict_kfold.items():\n",
        "\n",
        "\n",
        "\n",
        "    #k='4'\n",
        "    #v=dict_kfold[k]\n",
        "    train_X, train_y= X[v['Train'],:,:], y[v['Train'],:,:]\n",
        "    val_X, val_y=  X[v['Val'],:,:], y[v['Val'],:,:]\n",
        "    test_X, test_y =  X[v['Test'],:,:], y[v['Test'],:,:]\n",
        "\n",
        "    # reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape((train_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    val_X = val_X.reshape((val_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    test_X = test_X.reshape((test_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    print('Shape train set (X, y): ', train_X.shape, train_y.shape)\n",
        "    print('Shape val set (X, y): ', val_X.shape, val_y.shape)\n",
        "    print('Shape test set (X, y): ', test_X.shape, test_y.shape)\n",
        "\n",
        "    # fit network\n",
        "    #history = model.fit(train_X.astype('float32'), train_y.astype('float32'), epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_X.astype('float32'), val_y.astype('float32')), verbose=2, shuffle=False)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=PATIENCE,\n",
        "                                                      mode='min') \n",
        "    if iter==0:\n",
        "      model=MODEL(N_LAGS, N_FEATURES, PH)    \n",
        "\n",
        "    history = model.fit(train_X.astype('float32'), train_y.astype('float32'), \n",
        "                          epochs=MAX_EPOCHS, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          validation_data=(val_X.astype('float32'), val_y.astype('float32')), \n",
        "                          #verbose=2, shuffle=False,\n",
        "                          callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores_val= model.evaluate(val_X.astype('float32'), val_y.astype('float32'), verbose=1)\n",
        "    scores_test = model.evaluate(test_X.astype('float32'), test_y.astype('float32'), verbose=1)\n",
        "\n",
        "    print('Fold K (Val): ', (model.metrics_names[0], scores_val*100))\n",
        "    print('Fold K (Test): ', (model.metrics_names[0], scores_test*100))\n",
        "\n",
        "    cvscores_val.append(scores_val)\n",
        "    cvscores_test.append(scores_test)\n",
        "\n",
        "    # plot history\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='val')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "    #Training part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(train_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, train_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    train_y = train_y.reshape((len(train_y), PH))\n",
        "    inv_y = np.concatenate((train_y, train_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "    # calculate RMSE\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Train RMSE:', rmse)\n",
        "    print('Train MAE: ', mae_)\n",
        "    print('Train MAPE(%): ', mape_)\n",
        "    Train_RMSE.append(rmse)\n",
        "    Train_MAE.append(mae_)\n",
        "    Train_MAPE.append(mape_)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(25,8))\n",
        "    plt. plot(inv_y[:-1], label='Train')\n",
        "    plt.plot(inv_yhat[1:], label= 'Estimated')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #Valdidation part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(val_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    val_X = val_X.reshape((val_X.shape[0], val_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, val_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    val_y = val_y.reshape((len(val_y), PH))\n",
        "    inv_y = np.concatenate((val_y, val_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "    # calculate RMSE\n",
        "    #rmse = mean_squared_error(inv_y[:-1], inv_yhat[1:])\n",
        "    #mae_ = mean_absolute_error(inv_y[:-1], inv_yhat[1:])\n",
        "    #mape_=mean_absolute_percentage_error(inv_y[:-1], inv_yhat[1:])\n",
        "\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Val RMSE: ',rmse)\n",
        "    print('Val MAE: ', mae_)\n",
        "    print('Val MAPE(%): ', mape_)\n",
        "    Val_RMSE.append(rmse)\n",
        "    Val_MAE.append(mae_)\n",
        "    Val_MAPE.append(mape_)\n",
        "\n",
        "    plt.figure()\n",
        "    plt. plot(inv_y[:-1], label='Val')\n",
        "    plt.plot(inv_yhat[1:], label= 'Estimated')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    #Test part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(test_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, test_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    test_y = test_y.reshape((len(test_y), PH))\n",
        "    inv_y = np.concatenate((test_y, test_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "\n",
        "\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Test RMSE: ',rmse)\n",
        "    print('Test MAE: ', mae_)\n",
        "    print('Test MAPE(%): ', mape_)\n",
        "    Test_RMSE.append(rmse)\n",
        "    Test_MAE.append(mae_)\n",
        "    Test_MAPE.append(mape_)\n",
        "\n",
        "    plt.figure()\n",
        "    plt. plot(inv_y[:-1], label='Test')\n",
        "    plt.plot(inv_yhat[1:], label= 'Prediction')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  # Save model\n",
        "  model.save_weights(PATH+'weights_AE_'+MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)+'.h5')\n",
        "  \n",
        "  # Save metrics\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]={}\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Train']=(Train_RMSE, Train_MAE,Train_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Val']=(Val_RMSE, Val_MAE,Val_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Test']=(Test_RMSE, Test_MAE,Test_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Theil']= theil (inv_y, inv_yhat, test_X,PH)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Pocid']= pocid(inv_yhat,inv_y)\n",
        "\n",
        "  pickle_out = open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\",\"wb\")\n",
        "  pickle.dump(res_barrido, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb7_G4chfigL"
      },
      "source": [
        "#for PH in LIST_PH:\n",
        "#  for N_LAGS in LIST_N_LAGS:\n",
        "#    MODEL= model_1\n",
        "#    MODEL_LABEL='M1'\n",
        "#    with open(PATH+\"pickle_res_barrido.pickle\", \"rb\") as input_file:\n",
        "#      res_barrido = pickle.load(input_file)\n",
        "#    main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKnqZNtewnz-"
      },
      "source": [
        "### Model 01\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb-oy0PbtbNu"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS[1:]:\n",
        "    MODEL= model_01\n",
        "    MODEL_LABEL='M1'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwRCjeV8vi-g"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWEzfGQI9nRm"
      },
      "source": [
        "res_barrido.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRPnsw0Ewtyr"
      },
      "source": [
        "### Model 02\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROBBrUYMG-BG"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_02\n",
        "    MODEL_LABEL='M2'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsxJzux3wtzK"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GY7QMZOOpoR"
      },
      "source": [
        "### Model 03\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYERdhGgOpog"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_03\n",
        "    MODEL_LABEL='M3'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWfwKhrwOpoi"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7vrP8zjMRSN"
      },
      "source": [
        "### Model 04\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6S-_tShMRSk"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_04\n",
        "    MODEL_LABEL='M4'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlB2AxSUMRSr"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHKFWSsOu-q"
      },
      "source": [
        "### Model 05\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ViIbPJDOu-s"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_05\n",
        "    MODEL_LABEL='M5'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x6YgTMSOu-u"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cifRSenHeJCp"
      },
      "source": [
        "res_barrido.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cXDDr1OviK"
      },
      "source": [
        "### Model 06\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qmn0ilopOviL"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_06\n",
        "    MODEL_LABEL='M6'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CAK5NeCOviM"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twx21faimwjB"
      },
      "source": [
        "### Model 06 (II)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb-qamKxmwjM"
      },
      "source": [
        "\n",
        "PH=3\n",
        "N_LAGS=180\n",
        "MODEL= model_06\n",
        "MODEL_LABEL='M6'\n",
        "print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "try:\n",
        "  main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS)\n",
        "except:\n",
        "  print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWO_B2UmwjR"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbeJtwPpwbF"
      },
      "source": [
        "np.asarray([x for x in res_barrido.keys()])[:,None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAixayyuWZ_x"
      },
      "source": [
        "# Barrido N_LAGS and PH modelos (Regularización)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuf2pvK0WZ_6"
      },
      "source": [
        "## BARRIDO DE PARÁMETROS\n",
        "\n",
        "LIST_PH=[1,3,7,15]\n",
        "LIST_N_LAGS=[180,90,30,15]\n",
        "#LIST_N_LAGS=LIST_N_LAGS[1:]\n",
        "\n",
        "# PREDICTION PARAMS (no- change!)\n",
        "#PH= 3\n",
        "#N_LAGS= 180#30\n",
        "MAX_EPOCHS = 20\n",
        "BATCH_SIZE=32\n",
        "PATIENCE=2\n",
        "OPTIMIZER='adam'\n",
        "lOSS_METRIC='mae'\n",
        "from keras.regularizers import L1L2\n",
        "REGULARIZERS = [L1L2(l1=0.01, l2=0.0), L1L2(l1=0.0, l2=0.01), L1L2(l1=0.01, l2=0.01)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxAmRIWXY9B3"
      },
      "source": [
        "# univariate multi-step encoder-decoder lstm example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjELkf1NWZ_8"
      },
      "source": [
        "#Cargar librerías\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Bidirectional\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMTzyumzWZ_9"
      },
      "source": [
        "## mAIN PROCESS\n",
        "\n",
        "def main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS,reg):\n",
        "  #Prepar data k-validation\n",
        "  # PREDICTION PARAMS\n",
        "  OUT_STEPS=PH\n",
        "  num_features=N_FEATURES\n",
        "  #\n",
        "\n",
        "  #porcessed signal\n",
        "  X, y = split_sequence(list(data_scaled), N_LAGS, PH)\n",
        "  data_transf=series_to_supervised(data_scaled, n_in=N_LAGS, n_out=PH, dropnan=True)\n",
        "\n",
        "  ## Prepare data for 4-fold validation (method 1)\n",
        "\n",
        "  # division train/val/test\n",
        "  len_h=len(X)\n",
        "  dict_kfold={'4': {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)} }\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['4']['Test'])\n",
        "  dict_kfold ['3']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['3']['Test'])\n",
        "  dict_kfold['2']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "  print(len_h)\n",
        "  len_h=len_h-len(dict_kfold['2']['Test'])\n",
        "  dict_kfold ['1']= {'Train': np.arange(0, int(per_train*len_h)),\n",
        "                    'Val': np.arange(int(per_train*len_h), int(per_train*len_h)+int(per_val*len_h)), \n",
        "                    'Test':np.arange(int(per_train*len_h)+int(per_val*len_h), len_h)}\n",
        "\n",
        "\n",
        "\n",
        "  # Model architecture \n",
        "  #MODEL= model_1\n",
        "  #MODEL_LABEL='M1'\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]={}\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Train']=[]\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Val']=[]\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Test']=[]\n",
        "\n",
        "  #TRAINING MODEL\n",
        "  cvscores_val = []\n",
        "  cvscores_test=[]\n",
        "  iter=0\n",
        "\n",
        "\n",
        "  Train_RMSE=[]\n",
        "  Train_MAE=[]\n",
        "  Train_MAPE=[]\n",
        "  Val_RMSE=[]\n",
        "  Val_MAE=[]\n",
        "  Val_MAPE=[]\n",
        "  Test_RMSE=[]\n",
        "  Test_MAE=[]\n",
        "  Test_MAPE=[]\n",
        "\n",
        "\n",
        "  for k, v in dict_kfold.items():\n",
        "\n",
        "\n",
        "\n",
        "    #k='4'\n",
        "    #v=dict_kfold[k]\n",
        "    train_X, train_y= X[v['Train'],:,:], y[v['Train'],:,:]\n",
        "    val_X, val_y=  X[v['Val'],:,:], y[v['Val'],:,:]\n",
        "    test_X, test_y =  X[v['Test'],:,:], y[v['Test'],:,:]\n",
        "\n",
        "    # reshape input to be 3D [samples, timesteps, features]\n",
        "    train_X = train_X.reshape((train_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    val_X = val_X.reshape((val_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    test_X = test_X.reshape((test_X.shape[0], N_LAGS, N_FEATURES))\n",
        "    print('Shape train set (X, y): ', train_X.shape, train_y.shape)\n",
        "    print('Shape val set (X, y): ', val_X.shape, val_y.shape)\n",
        "    print('Shape test set (X, y): ', test_X.shape, test_y.shape)\n",
        "\n",
        "    # fit network\n",
        "    #history = model.fit(train_X.astype('float32'), train_y.astype('float32'), epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_X.astype('float32'), val_y.astype('float32')), verbose=2, shuffle=False)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=PATIENCE,\n",
        "                                                      mode='min') \n",
        "    if iter==0:\n",
        "      model=MODEL(N_LAGS, N_FEATURES, PH,reg)    \n",
        "\n",
        "    history = model.fit(train_X.astype('float32'), train_y.astype('float32'), \n",
        "                          epochs=MAX_EPOCHS, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          validation_data=(val_X.astype('float32'), val_y.astype('float32')), \n",
        "                          #verbose=2, shuffle=False,\n",
        "                          callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores_val= model.evaluate(val_X.astype('float32'), val_y.astype('float32'), verbose=1)\n",
        "    scores_test = model.evaluate(test_X.astype('float32'), test_y.astype('float32'), verbose=1)\n",
        "\n",
        "    print('Fold K (Val): ', (model.metrics_names[0], scores_val*100))\n",
        "    print('Fold K (Test): ', (model.metrics_names[0], scores_test*100))\n",
        "\n",
        "    cvscores_val.append(scores_val)\n",
        "    cvscores_test.append(scores_test)\n",
        "\n",
        "    # plot history\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='val')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "    #Training part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(train_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, train_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    train_y = train_y.reshape((len(train_y), PH))\n",
        "    inv_y = np.concatenate((train_y, train_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "    # calculate RMSE\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Train RMSE:', rmse)\n",
        "    print('Train MAE: ', mae_)\n",
        "    print('Train MAPE(%): ', mape_)\n",
        "    Train_RMSE.append(rmse)\n",
        "    Train_MAE.append(mae_)\n",
        "    Train_MAPE.append(mape_)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(25,8))\n",
        "    plt. plot(inv_y[:-1], label='Train')\n",
        "    plt.plot(inv_yhat[1:], label= 'Estimated')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #Valdidation part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(val_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    val_X = val_X.reshape((val_X.shape[0], val_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, val_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    val_y = val_y.reshape((len(val_y), PH))\n",
        "    inv_y = np.concatenate((val_y, val_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "    # calculate RMSE\n",
        "    #rmse = mean_squared_error(inv_y[:-1], inv_yhat[1:])\n",
        "    #mae_ = mean_absolute_error(inv_y[:-1], inv_yhat[1:])\n",
        "    #mape_=mean_absolute_percentage_error(inv_y[:-1], inv_yhat[1:])\n",
        "\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Val RMSE: ',rmse)\n",
        "    print('Val MAE: ', mae_)\n",
        "    print('Val MAPE(%): ', mape_)\n",
        "    Val_RMSE.append(rmse)\n",
        "    Val_MAE.append(mae_)\n",
        "    Val_MAPE.append(mape_)\n",
        "\n",
        "    plt.figure()\n",
        "    plt. plot(inv_y[:-1], label='Val')\n",
        "    plt.plot(inv_yhat[1:], label= 'Estimated')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    #Test part\n",
        "    # make a prediction\n",
        "    yhat = model.predict(test_X)\n",
        "    yhat=yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
        "    # invert scaling for forecast\n",
        "    inv_yhat = np.concatenate((yhat, test_X), axis=1)\n",
        "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "    inv_yhat = inv_yhat[:,:PH]\n",
        "    # invert scaling for actual\n",
        "    test_y = test_y.reshape((len(test_y), PH))\n",
        "    inv_y = np.concatenate((test_y, test_X), axis=1)\n",
        "    inv_y = scaler.inverse_transform(inv_y)\n",
        "    inv_y = inv_y[:,:PH]\n",
        "\n",
        "\n",
        "    rmse = mean_squared_error(inv_y, inv_yhat)\n",
        "    mae_ = mean_absolute_error(inv_y, inv_yhat)\n",
        "    mape_=mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "    print('Test RMSE: ',rmse)\n",
        "    print('Test MAE: ', mae_)\n",
        "    print('Test MAPE(%): ', mape_)\n",
        "    Test_RMSE.append(rmse)\n",
        "    Test_MAE.append(mae_)\n",
        "    Test_MAPE.append(mape_)\n",
        "\n",
        "    plt.figure()\n",
        "    plt. plot(inv_y[:-1], label='Test')\n",
        "    plt.plot(inv_yhat[1:], label= 'Prediction')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  # Save model\n",
        "  model.save_weights(PATH+'weights_AE_'+MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)+'.h5')\n",
        "  \n",
        "  # Save metrics\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]={}\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Train']=(Train_RMSE, Train_MAE,Train_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Val']=(Val_RMSE, Val_MAE,Val_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Test']=(Test_RMSE, Test_MAE,Test_MAPE)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Theil']= theil (inv_y, inv_yhat, test_X,PH)\n",
        "  res_barrido[MODEL_LABEL+'_'+str(PH)+'_'+str(N_LAGS)]['Pocid']= pocid(inv_yhat,inv_y)\n",
        "\n",
        "  pickle_out = open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\",\"wb\")\n",
        "  pickle.dump(res_barrido, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6X_Nal3WaAT"
      },
      "source": [
        "### Model 02 - L1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odnyCSrZXS8O"
      },
      "source": [
        "## model 2\n",
        "# define the model\n",
        "def model_02(N_LAGS, N_FEATURES, PH,reg):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', recurrent_regularizer=reg, input_shape=(N_LAGS, N_FEATURES)))\n",
        "  model.add(RepeatVector(PH))\n",
        "  model.add(LSTM(100, activation='relu', recurrent_regularizer=reg, return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=lOSS_METRIC, optimizer=OPTIMIZER)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI1ZhmpOWaAU"
      },
      "source": [
        "\n",
        "for PH in LIST_PH:\n",
        "  for N_LAGS in LIST_N_LAGS:\n",
        "    MODEL= model_02\n",
        "    MODEL_LABEL='M2_regL1'\n",
        "    print(MODEL_LABEL, str(N_LAGS),str(PH))\n",
        "    with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "      res_barrido = pickle.load(input_file)\n",
        "    try:\n",
        "      main_process(data_scaled, res_barrido, MODEL, MODEL_LABEL,PH,N_LAGS,REGULARIZERS[0])\n",
        "    except:\n",
        "      print('Error in: ', MODEL_LABEL, str(N_LAGS))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbB2RHh-WaAW"
      },
      "source": [
        "with open(PATH+\"Definitive_scripts/\"+\"pickle_res_barrido_AE.pickle\", \"rb\") as input_file:\n",
        "  res_barrido = pickle.load(input_file)\n",
        "\n",
        "res_barrido"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}